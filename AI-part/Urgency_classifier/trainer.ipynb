{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel , AutoTokenizer, AdamW"
      ],
      "metadata": {
        "id": "LrLJUeII2Jv1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXWzdOXM2OvF",
        "outputId": "f10c178f-4cfe-4f4e-ab51-6d8f46cc2bae"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "1iIN-Nly2YP6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Qv7WDUNq3mUL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Untitled 1.csv')"
      ],
      "metadata": {
        "id": "HU-JyvfH2z0G"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "TALCwNtU4Umj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['Priority'])\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['Priority'])"
      ],
      "metadata": {
        "id": "3Dqcbjpy4JRe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {'Low': 0, 'Medium': 1, 'High': 2}\n",
        "train_df['label'] = train_df['Priority'].map(label_map)\n",
        "val_df['label'] = val_df['Priority'].map(label_map)\n",
        "test_df['label'] = test_df['Priority'].map(label_map)"
      ],
      "metadata": {
        "id": "tGtH5chP4UEE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "274ti_lr6YQ4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "H8VYfjM_7Sfg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "from datasets import Dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['Text'], padding=\"max_length\", truncation=True, max_length=128)\n",
        "train_dataset = Dataset.from_pandas(train_df[['Text', 'label']])\n",
        "val_dataset = Dataset.from_pandas(val_df[['Text', 'label']])\n",
        "test_dataset = Dataset.from_pandas(test_df[['Text', 'label']])\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "train_tf_dataset = train_dataset.to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\"],\n",
        "    label_cols=\"label\",\n",
        "    batch_size=16\n",
        ")\n",
        "val_tf_dataset = val_dataset.to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\"],\n",
        "    label_cols=\"label\",\n",
        "    batch_size=16\n",
        ")\n",
        "test_tf_dataset = test_dataset.to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\"],\n",
        "    label_cols=\"label\",\n",
        "    batch_size=16\n",
        ")"
      ],
      "metadata": {
        "id": "X06l8Hgp7kpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(train_tf_dataset, epochs=50, validation_data=val_tf_dataset)\n"
      ],
      "metadata": {
        "id": "G0fnfP9o8FOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_tf_dataset)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "r_sfSUMZCGLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model\")"
      ],
      "metadata": {
        "id": "WIFqQEeXMq08"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}